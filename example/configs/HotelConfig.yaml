
name: "TransformerEncoder" # Model name


task: "HotelReservationTask" # HotelReservationTask or BuyChargeTask
seed: 11111


model:
  layer1:
    layer: "input" # layer name
    shape: ${model_params.max_seq_len} # layer param
  layer2:
    layer: "TokenAndPositionEmbedding" # layer name
    vocabulary_size: ${model_params.vocab_size} # layer param
    sequence_length: ${model_params.max_seq_len} # layer param
    embedding_dim: ${model_params.embedding_dim} # layer param
  layer3:
    layer: "dropout" # layer name
    rate: 0.2 # layer param
  layer4:
    layer: "TransformerEncoder" # layer name
    intermediate_dim: 200 # layer param
    num_heads: 2 # layer param
    dropout: 0.1 # layer param
    activation: "relu" # layer param
    layer_norm_epsilon: 1e-05 # layer param
  layer5:
    layer: "dense" # layer name
    activation: "gelu" # layer param
    dim: 150 # layer param
    time_distributed: true # layer param
  layer6:
    layer: "dense" # layer name
    activation: "softmax" # layer param
    dim: 13  # layer param, 13 if data.add_sos_eos = false else 15
    time_distributed: true # layer param




model_params:
  max_seq_len: 45
  num_words: 250
  vocab_size: 300 # "num_words" + 50
  batch_size: 32
  epochs: 3
  steps_per_epoch: 5000
  validation_steps: 3000
  embedding_dim: 300
  # for now only Adam optimizer is supported. More optimizers will be added if needed...
  optimizer:
    type: "adam"
    learning_rate: 0.001
  # for now only categorical_crossentropy loss is supported. More optimizers will be added if needed...
  loss: "categorical_crossentropy"
  metrics: ["precision", "recall", "accuracy"]

callbacks:
  ModelCheckpoint:
    filepath: /media/SSD1TB/khayati/projects/nlu/intent_slot_filling/output/model/${name}/model_weights.best.hdf5
    monitor: "val_loss"
    verbose: 1
    save_best_only: True
    save_weights_only: True
    mode: "min"
    save_freq: "epoch"


  ReduceLROnPlateau:
    monitor: "val_loss"
    factor: 0.8
    patience: 2
    verbose: 1
    mode: "auto"
    min_delta: 0.0001
    cooldown: 1
    min_lr: 0.0001

  TensorBoard:
    log_dir: /media/SSD1TB/khayati/projects/nlu/intent_slot_filling/output/logs/${name}
    histogram_freq: 0
    write_graph: true
    write_images: false
    write_steps_per_second: false
    update_freq: "epoch"
    profile_batch: 0
    embeddings_freq: 0
    embeddings_metadata: None

save_model_dir: /media/SSD1TB/khayati/projects/nlu/intent_slot_filling/output/model/${name}


data:
  # if you are loading data from pickle files, their name must be
  # train_data.pickle, validation_data.pickle and test_data.pickle if use_test_set is True
  from: "pickle" # text or pickle
  # Path of directory where templates.txt or train_task.pickle, validation_task.pickle, and test_task.pickle files are there.
  data_dir: "/media/SSD1TB/khayati/projects/nlu/intent_slot_filling/data"
  save_data_dir: /media/SSD1TB/khayati/projects/nlu/intent_slot_filling/output/data_dump/${name}
  # data split ratio
  split: 0.2
  # if true, "2*split" percent of data is taken away for validation and test set and the rest is for train.
  # else, "split" percent of data is taken away for validation and the rest is for train.
  use_test_set: false
  add_sos_eos: false # Add <sos> and <eos> at the beginning and end of sentence
  augmentation: 0.5 # Augemntation Percent,  set 0 for no augmentation
  labels: {
    "PAD": 0,
    "O": 1,
    "num_room": 2,
    "time": 3,
    "time_unit": 4,
    "num_bed": 5,
    "num_person": 6,
    "person_unit": 7,
    "hotel_name": 8,
    "hotel_city": 9,
    "person_unit_post": 10,
    "hotel_name_post": 11,
    "hotel_city_post": 12,
    "<sos>": 13,
    "<eos>": 14 }








